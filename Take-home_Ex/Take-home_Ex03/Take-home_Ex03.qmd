---
title: "Take-home Exercise 3"
author: "Alicia"

date: "3 Jun 2023"
date-modified: "`r Sys.Date()`"
editor: visual

format:
  html:
    code-fold: true
    code-summary: "Show the code"
    
execute:
  echo: true
  eval: true
  warning: false
---

# **1. Overview**

This exercise aims to use appropriate static and interactive statistical graphics methods to help FishEye identify companies that may be engaged in illegal fishing.

The original dataset was originated from Mini Challenge 3 of Vast Challenge 2023.

There is one file downloaded: *MC3.json*. 

This exercise aims to answer Q1 of the challenge:

-   Use visual analytics to identify anomalies in the business groups present in the knowledge graph. Limit your response to 400 words and 5 images.

# **2. Getting Started**

## 2.1 Installing and launching R packages

The code chunk below will be used to install and load the necessary R packages to meet the data preparation, data wrangling, data analysis and visualisation needs.

```{r}
pacman::p_load(jsonlite, tidygraph, ggraph, 
               visNetwork, graphlayouts, ggforce, 
               skimr, tidytext, tidyverse, extrafont)
```

## 2.2 Importing json file by using jsonlite package

```{r}
mc3_data <- fromJSON("data/MC3.json")
```

## 2.3 Data Preparation

### 2.3.1 Extracting edges

The code chunk below will be used to extract the links data.frame of mc3_data and save it as a tibble data.frame called mc3_edges.

```{r}
mc3_edges <- as_tibble(mc3_data$links) %>% 
  distinct() %>%
  mutate(source = as.character(source),
         target = as.character(target),
         type = as.character(type)) %>%
  group_by(source, target, type) %>%
    summarise(weights = n()) %>%
  filter(source!=target) %>%
  ungroup()
```

### 2.3.2 Extracting nodes

The code chunk below will be used to extract the nodes data.frame of mc3_data and save it as a tibble data.frame called mc3_nodes.

```{r}
mc3_nodes <- as_tibble(mc3_data$nodes) %>%
  mutate(country = as.character(country),
         id = as.character(id),
         product_services = as.character(product_services),
         revenue_omu = as.numeric(as.character(revenue_omu)),
         type = as.character(type)) %>%
  select(id, country, type, revenue_omu, product_services)
```

# **3. Initial Data Exploration**

## 3.1 Exploring the edges data frame

In the code chunk below, skim() of skimr package is used to display the summary statistics of mc3_edges tibble data frame.

```{r}
skim(mc3_edges)
```

The report above reveals that there is not missing values in all fields.

In the code chunk below, datatable() of DT package is used to display mc3_edges tibble data frame as an interactive table on the html document.

```{r}
DT::datatable(mc3_edges)
```

A plot below shows the distribution of variable type.

```{r}
ggplot(data = mc3_edges,
       aes(x = type)) +
  geom_bar()
```

## 3.2 Initial Network Visualisation and Analysis

### 3.2.1 Building network model with tidygraph

```{r}
id1 <- mc3_edges %>%
  select(source) %>%
  rename(id = source)
id2 <- mc3_edges %>%
  select(target) %>%
  rename(id = target)
mc3_nodes1 <- rbind(id1, id2) %>%
  distinct() %>%
  left_join(mc3_nodes,
            unmatched = "drop")

mc3_graph <- tbl_graph(nodes = mc3_nodes1,
                       edges = mc3_edges,
                       directed = FALSE) %>%
  mutate(betweenness_centrality = centrality_betweenness(),
         closeness_centrality = centrality_closeness())

mc3_graph %>%
  filter(betweenness_centrality >= 100000) %>%
ggraph(layout = "fr") +
  geom_edge_link(aes(alpha=0.5)) +
  geom_node_point(aes(
    size = betweenness_centrality,
    colors = "lightblue",
    alpha = 0.5)) +
  scale_size_continuous(range=c(1,10))+
  theme_graph()
```

## 3.3 Exploring the nodes data frame

In the code chunk below, skim() of skimr package is used to display the summary statistics of mc3_nodes tibble data frame.

```{r}
skim(mc3_nodes)
```

The report above reveals that there is no missing values in all fields.

In the code chunk below, datatable() of DT package is used to display mc3_nodes tibble data frame as an interactive table on the html document.

```{r}
DT::datatable(mc3_nodes)
```

A plot below shows the distribution of variable type.

```{r}
ggplot(data = mc3_nodes,
       aes(x = type)) +
  geom_bar()
```

## 3.4 Text Sensing with tidytext

This section performs basic text sensing using appropriate functions of tidytext package.

### 3.4.1 Simple word count

```{r}
mc3_nodes %>% 
    mutate(n_fish = str_count(product_services, "fish")) 
```

### 3.4.2 Tokenisation

The word tokenisation have different meaning in different scientific domains. In text sensing, tokenisation is the process of breaking up a given text into units called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenisation, some characters like punctuation marks may be discarded. The tokens usually become the input for the processes like parsing and text mining.

In the code chunk below, unnest_token() of tidytext is used to split text in product_services field into words.

```{r}
token_nodes <- mc3_nodes %>%
  unnest_tokens(word, 
                product_services)
```

The two basic arguments to unnest_tokens() used here are column names. First we have the output column name that will be created as the text is unnested into it (word, in this case), and then the input column that the text comes from (product_services, in this case).

Next, we visualise the words extracted by using the code chunk below.

```{r}
token_nodes %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in product_services field")
```

The bar chart reveals that the unique words contains some words that may not be useful to use e.g. “and” and “of”. We want to remove these words from your analysis as they are fillers used to compose a sentence.

### 3.4.3 Removing stopwords

Use tidytext package that has a function called stop_words that will help to clean up stop words.

```{r}
stopwords_removed <- token_nodes %>% 
  anti_join(stop_words)

stopwords_removed %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in product_services field")
```

### 3.4.4 Recode words "character", "0", and "unknown" in product_services field to NA 

To make the data more clean and meaningful, we recode the words "character", "0", and "unknown" in product_services field to NA.

```{r}
stopwords_removed$word[grepl("character|characters|characterization|0|unknown", stopwords_removed$word)]<-"NA"
```

### 3.4.5 Identify source-type and target-type

#```{r}
#mc3_edges %>% 
#left_join(mc3_nodes,
#            unmatched = "drop")
#```
  